---
title: "RKHS"
author: 
  - Costa, W. G.^[Weverton Gomes da Costa, Pós-Doutorando, Embrapa Mandioca e Fruticultura, wevertonufv@gmail.com]
date: "`r Sys.Date()`"
site: workflowr::wflow_site
url: https://wevertongomescosta.github.io/Genomic-Selection-for-Drought-Tolerance-Using-Genome-Wide-SNPs-in-Casava
output:
  workflowr::wflow_html:
    toc: TRUE
editor_options:
  chunk_output_type: console
---

## Configurations and packages

```{r setup, include=F} 
knitr::opts_chunk$set(echo = T, warning = F)
```

To peRKHSorm the analyses, we will need the following packages:

```{r libraries, message=FALSE}
library(furrr)
library(tidyverse)
library(cvTools)
library(kableExtra)
```

## Data

```{r data}
geno<- readRDS("data/geno.rds")
pheno<-readRDS("data/pheno.rds")
traits <- colnames(pheno)[-1]
```

## RKHS



We can also use the eigenvalues of the G covariance matrix Gaussian Kernel (GK) to perform the analyses. But first, we have to build GK.

For this purpose, we will use the [sommer](https://cran.r-project.org/web/packages/sommer/vignettes/v3.sommer.qg.pdf) package [@Covarrubias-Pazaran_2014]. In the code below, *y* is for the vector with the means, *GK* is G covariance matrix Gaussian Kernel for the random effects, which will be and identity matrix by default; and *X* is a design matrix for the fixed effects, which will be a vector of ones (**1**) by default. Note that we are returning to the "1, 0, -1" codification in the SNPs matrix. This is a requirement of the _sommer_ package.

```{r}
dist <- as.matrix(dist(scale(geno, center = TRUE, scale = TRUE))) ^ 2
GK <- exp(-dist / median(dist))
GK[1:5,1:5]
```

### Cross-validation 

To prove that the prediction is accurate, we should peRKHSorm a cross-validation (CV) scheme. For this purpose, we divide the data into a training set and a validation set. 

First we separate the data into *k* folds. Then, we attribute *NA* for one fold and try to predict the data from this fold based on the others. When selecting the number of folds, one must prioritize the balance between the number of observations in each fold. 

In addition, this process should be repeated for further validation. The step-by-step below will guide the CV in the data we are analysing.

####  1. Determine the number of folds and repetitions

```{r nfolds-nrep}
nfolds = 5
nrept = 5
```

Since we defined 5 folds, our data will be divided into 5 parts with 83 observations each.

#### 2. Match the order of the data and the rows of the SNP matrix

The order is decreasing or increasing (numeric or alphabetical) regarding the name of the genotypes.

```{r order-data}
pheno <- pheno[order(pheno$ID_Clone, decreasing = F),]
geno <- geno[order(row.names(geno)),]
all(rownames(geno) == pheno$ID_Clone)
```

#### 3. Add a column indicating a number for each observation

This will be useful to assign each observation for a fold, which will be the next step.

```{r add-column}
pheno$ID <- factor(1:nrow(pheno))
```

#### 4. Folds assignment

In this step, we will assign each observation to a fold. Bear in mind that for each repetition, the folds will comprise different observations. The purpose of the repetition is to make sure of the randomness of the assignment step. 

In this step, we will use the [cvTools](https://cran.r-project.org/web/packages/cvTools/cvTools.pdf) package [@cvTools]

For better peRKHSormance, we included this step in the loop of the next step.

### Run model

The next step is the very CV. Here, we will define the linear predictor and the lists that will be useful in the loop. The first list, here called results_cv_RKHS and, contains the results for each fold assignation that we built in the previous step. The second GEBVS_RKHS contains the GEBVs.

Then, we will construct the loop. Each iteration will assign *NA* for a different fold, and we will use the other folds to predict the missing values. Note that the folds vary for each repetition and trait.

Set up parallel processing. Define and run multiple models to get the GEBVs *ATTENTION: This process is time-consuming.*

```{r RKHS}
# Planejar o processamento em várias sessões para paralelização
plan(multisession)

# Iniciar uma lista para armazenar os resultados
results_list <-
  future_map(traits, .options = furrr_options(seed = 100), function(j)  {
    # Pré-processamento dos dados
    pheno2 <- pheno %>%
      select(ID_Clone, all_of(j)) %>%
      na.omit() %>%
      ungroup() %>%
      filter(ID_Clone %in% rownames(GK)) %>%
      mutate(ID = factor(1:nrow(.))) %>%
      droplevels()
    
    # Filtrar as linhas de Z com base em pheno2$ID_Clone
    GK1 <-
      GK[rownames(GK) %in% pheno2$ID_Clone, rownames(GK) %in% pheno2$ID_Clone]
    
    #Semente para reprodução
    set.seed(100)
    
    # Listas das folds
    fold.list <- lapply(1:nrept, function(a) {
      folds <- cvFolds(nlevels(pheno2$ID),
                       type = "random",
                       K = nfolds)
      
      split(folds$subsets, f = folds$which)
    })
    
    # Usar future_map_dfr para criar dobras cruzadas em paralelo
    future_map_dfr(1:length(fold.list), function(z) {
      for (i in 1:nfolds) {
        # Conjunto de treinamento
        train_data <- pheno2
        
        # Conjunto de validação
        train_data[train_data$ID %in% fold.list[[z]][[i]], j] <- NA
        
        # Fitting model
        fixed <- formula(paste(j, " ~ 1"))
        
        RKHS <-
          sommer::mmer(
            fixed,
            random = ~ sommer::vsr(ID_Clone, Gu = GK1),
            data = train_data,
            rcov = ~ units
          )
        
        # GETGV
        Pred <-
          data.frame(
            Yhat =  RKHS$U$`u:ID_Clone`[[j]],
            ID = train_data$ID,
            ID_Clone = train_data$ID_Clone
          ) %>%
          filter(ID %in% fold.list[[z]][[i]])
        
        result1 <-
          tibble(
            GEBV = Pred$Yhat,
            ID = Pred$ID,
            ID_Clone = Pred$ID_Clone,
            Trait = j,
            rep = z,
            fold = i
          )
        
        
        # Resultados da previsão
        if (i == 1) {
          result <- result1
        } else {
          result <- rbind(result, result1)
        }
      }
      
      #Ordenando os resultados de acordo com o ID
      result <- result[order(result$ID, decreasing = F),]
      
      # Resultados da validação cruzada
      result_cv <- tibble(
        Trait = j,
        rep = z,
        Log = all(result$ID == pheno2$ID),
        Ac = round(cor(result$GEBV, pheno2[[j]], use = "na.or.complete"), 3),
        MSPE = round(mean((
          result$GEBV - pheno2[[j]]
        ) ^ 2, na.rm = TRUE), 3),
        result = list(result)
      )
    })
  })

# Combinar todos os resultados da validação cruzada em uma única estrutura
results_cv_RKHS <- do.call(rbind, results_list)

# Extrair os GEBVs (Valores Genômicos Empíricos Melhorados) em uma estrutura separada
GEBVS_RKHS <- do.call(rbind, results_cv_RKHS$result)
```

### Results

The object "GEBVS_RKHS" contains the GEBVs for each trait and clone. The table above shows the first five clones and their GEBVs for each trait.

```{r GEBVS_RKHS}
GEBVS_RKHS |>
  group_by(Trait, ID_Clone) |>
  summarise(GEBV = mean(GEBV)) %>%
  pivot_wider(names_from = Trait, values_from = GEBV) %>%
  head() %>%
  kbl(escape = F, align = 'c') |>
  kable_classic("hover", full_width = F, position = "center", fixed_thead = T)
```

```{r save-GEBVS_RKHS,  include=FALSE}
saveRDS(GEBVS_RKHS, "output/GEBVS_RKHS.RDS")
```

Acuracy and MSPE for each trait and repetition:

```{r results_cv_RKHS}
results_cv_RKHS[-6] |>
  group_by(Trait) %>% 
  summarise_all(mean) %>% 
  arrange(desc(Ac), MSPE) %>% 
  kbl(escape = F, align = 'c') |>
  kable_classic("hover", full_width = F, position = "center", fixed_thead = T)
```

```{r save-results_cv_RKHS,  include=FALSE}
saveRDS(results_cv_RKHS, "output/results_cv_RKHS.RDS")
```

The object "result_cv" is divided by repetition. In the "result_cv" objects for each repetition, "Rep" is the number of the repetition, "Log" is a diagnostic indicating if the order of the predicted breeding values matches the order of the adjusted means, "Ac" is the prediction accuracy (correlation between the GEBV and adjusted means), and "MSPE" is the mean square prediction error (the lower, the better).

Let's now move on to processing each model. Since this can be time-consuming, I've separated it into parallel processing and each model into each script:

- [*RR-BLUP**](https://wevertongomescosta.github.io/Genomic-Selection-for-Drought-Tolerance-Using-Genome-Wide-SNPs-in-Casava/GWS_RR_BLUP.html)
- [**G-BLUP (additive and additive-dominant)**](https://wevertongomescosta.github.io/Genomic-Selection-for-Drought-Tolerance-Using-Genome-Wide-SNPs-in-Casava/GWS_G-BLUP.html)
- [**Bayes A**](https://wevertongomescosta.github.io/Genomic-Selection-for-Drought-Tolerance-Using-Genome-Wide-SNPs-in-Casava/GWS_BayesA.html)
- [**Bayes B**](https://wevertongomescosta.github.io/Genomic-Selection-for-Drought-Tolerance-Using-Genome-Wide-SNPs-in-Casava/GWS_BayesB.html)
- [**RKHS**](https://wevertongomescosta.github.io/Genomic-Selection-for-Drought-Tolerance-Using-Genome-Wide-SNPs-in-Casava/GWS_RKHS.html)
- [**Random Forest**](https://wevertongomescosta.github.io/Genomic-Selection-for-Drought-Tolerance-Using-Genome-Wide-SNPs-in-Casava/GWS_RKHS.html)
